Two-Tower Complete Ablation Study (12 Variants)
============================================================

Dataset: Amazon Video Games 2023
98,906 users | 26,354 items | 99.97% sparse

Version  Config                            HR@10   Verdict
-----------------------------------------------------------------
v1       InfoNCE baseline                  0.6195  baseline
v2       +MSE distillation                 0.6210  gradient drowned
v3       +cosine distillation              0.6195  structural blindness
v4       +title text                       0.6355  +0.016 (helped)
v4b      +text batch=1024                  0.6280  bigger batch hurt
v4c      +text lr=3e-4                     0.6290  no gain
v4-BPR   BPR+hard neg                      0.1520  collapsed
v5       +text +GRU                        0.6395  BEST (+0.004)
v5b      +rich text +GRU                   0.6385  noisy descriptions
v5c      +LightGCN init                    0.6330  scrambled by MLP
v6       +curriculum neg                   0.6355  hard negs hurt
v7       +text +GRU +CLIP images           0.6355  images no help
v8       FM-style additive                 0.6305  no MLP = no bottleneck

FM Gate Weights (what the model learned):
  User: ID=62%, GRU=28%, Features=10%
  Item: ID=54%, Features=22%, Text=23%

Key Finding: ID embeddings carry 2-3x more signal than all
content features combined on ultra-sparse data.

Final Leaderboard:
  Two-Tower v5:  HR@10=0.6395 | Full-rank HR@10=0.0270
  MF (BPR):     HR@10=0.6755 | Full-rank HR@10=0.0420
  LightGCN:     HR@10=0.7285 | Full-rank HR@10=0.0440

FAISS Retrieval Latency:
  Flat (exact):   310 us/query
  IVF (approx):   35 us/query
  HNSW (graph):   29 us/query
  Brute-force:    894 us/query
